{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "  <div style=\"flex: 1; max-width: 20%; padding-right: 10px;\">\n",
    "    <img src=\"./images/ChatLLM-logo.png\" width=250px>\n",
    "  </div>\n",
    "  <div style=\"flex: 2; max-width: 80%;\">\n",
    "    <center><h1>ChatLLM</h1></center>\n",
    "    <p>ChatLLM is a versatile conversational platform designed to streamline interactions with <u>multiple language models</u>. <br><br> It allows users to <b>switch seamlessly between various models</b>, such as OpenAI's GPT series, LLaMA, and Mistral, within a single conversation. This <u>eliminates the need to juggle multiple browser tabs</u>, enabling efficient exploration of each model's unique capabilities in one cohesive interface. <br><br> With ChatLLM, users can maintain a unified chat history while dynamically selecting the best model for their needs during the conversation.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Import all the necessary libraries to launch ChatLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import requests\n",
    "import subprocess\n",
    "\n",
    "import gradio as gr\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variables and API Validation\n",
    "\n",
    "Starts loading environment variables from a `.env` file inside the project folder\n",
    "\n",
    "Create the `.env` file with the following cell, open it and store inside it your API keys:\n",
    "<br>\n",
    "<br>‚û°Ô∏è OPENAI_API_KEY has to be setted with your OpenAI API key created from https://platform.openai.com/account\n",
    "<br>‚û°Ô∏è OLLAMA_API_KEY is already setted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! [ ! -f .env ] && touch .env && echo \"OPENAI_API_KEY=your_openai_api_key_here\"> .env && echo \"OLLAMA_API_KEY=http://localhost:11434/\">> .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the environment variables from the `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading environment variables...\n",
      "‚úÖ OpenAI API key found and valid.\n",
      "‚ùå ERROR: Ollama server is not running. Attempting to start it...\n",
      "‚è≥ Starting Ollama server... Waiting for it to be reachable.\n"
     ]
    }
   ],
   "source": [
    "print(\"‚è≥ Loading environment variables...\")\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "ollama_api_key = os.getenv('OLLAMA_API_KEY')\n",
    "\n",
    "# check OpenAI API key\n",
    "if not openai_api_key or not openai_api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"‚ùå ERROR: OpenAI API key invalid or missing.\")\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API key found and valid.\")\n",
    "\n",
    "# check Ollama server\n",
    "try:\n",
    "    response = requests.get(ollama_api_key, timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"‚úÖ Ollama server is already running.\")\n",
    "except requests.exceptions.RequestException:\n",
    "    print(\"‚ùå ERROR: Ollama server is not running. Attempting to start it...\")\n",
    "\n",
    "    try:\n",
    "        subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(\"‚è≥ Starting Ollama server... Waiting for it to be reachable.\")\n",
    "        \n",
    "        for _ in range(100):\n",
    "            try:\n",
    "                response = requests.get(ollama_api_key, timeout=5)\n",
    "                if response.status_code == 200:\n",
    "                    print(\"‚úÖ Ollama server started successfully.\")\n",
    "                    break\n",
    "                    \n",
    "            except requests.exceptions.RequestException:\n",
    "                pass\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå ERROR: Ollama command not found. Download Ollama from: https://ollama.com/download.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: Failed to start Ollama server. Exception: {e}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose your system prompt\n",
    "\n",
    "Write down all the necessary information your AI assistant should need to help you in your task. Write it inside a `.txt` file inside the folder `prompts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ System Prompt read successfully: You are a strict code reviewer who focuses on identifying inefficiencies and potential bugs in Python code.\n"
     ]
    }
   ],
   "source": [
    "PROMPT_PATH = 'prompts'\n",
    "my_system_prompt = 'system_prompt.txt'\n",
    "\n",
    "# read the prompt file\n",
    "file_path=os.path.join(PROMPT_PATH, my_system_prompt)\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        system_message = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: The file at {file_path} was not found.\")\n",
    "except IOError as e:\n",
    "    print(f\"‚ùå ERROR: An error occurred while reading the file: {e}\")\n",
    "else:\n",
    "    print(f\"‚úÖ System Prompt read successfully: {system_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "Imports necessary functions to build a message in the OpenAI format, call the different models and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages(message, history):\n",
    "    \n",
    "    # convert Gradio history to OpenAI message format\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_message\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # add message pairs from history\n",
    "    for user_msg, assistant_msg in history:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_msg\n",
    "            }\n",
    "        )\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": assistant_msg\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # add the current message\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return messages\n",
    "\n",
    "def chat(message, history, model, debug=False):\n",
    "    \n",
    "    if 'gpt' in model:\n",
    "        \n",
    "        # build messages\n",
    "        messages = build_messages(message, history)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"History is:\")\n",
    "            print(json.dumps(history, indent=4))\n",
    "            print(\"And messages is:\")\n",
    "            print(json.dumps(messages, indent=4))\n",
    "        \n",
    "        # call GPT\n",
    "        stream = openai.chat.completions.create(\n",
    "            model=model, \n",
    "            messages=messages, \n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        # return response\n",
    "        response = \"\"\n",
    "        for chunk in stream:\n",
    "            response += chunk.choices[0].delta.content or ''\n",
    "            yield history + [(message, response)]\n",
    "\n",
    "    elif 'llama' in model or 'mistral' in model or 'qwen' in model or 'deepseek' in model:\n",
    "        \n",
    "        # check model\n",
    "        try:\n",
    "            models = ollama.list()\n",
    "            model_names = [m.model.split(':')[0] for m in models.models]\n",
    "            \n",
    "            if model not in model_names:\n",
    "                print(f\"üö® Model {model} not found. Downloading...\")\n",
    "                ollama.pull(model)\n",
    "                print(f\"‚úÖ Model {model} downloaded successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"‚ùå ERROR: checking/downloading model lead to {e}\")\n",
    "\n",
    "        # build messages\n",
    "        messages = build_messages(message, history)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"History is:\")\n",
    "            print(json.dumps(history, indent=4))\n",
    "            print(\"And messages is:\")\n",
    "            print(json.dumps(messages, indent=4))\n",
    "        \n",
    "        # call Ollama\n",
    "        stream = ollama.chat(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        # return response\n",
    "        response = \"\"\n",
    "        for chunk in stream:\n",
    "            response += chunk.get('message', {}).get('content', '')\n",
    "            yield history + [(message, response)]\n",
    "\n",
    "def clear_input():\n",
    "        return gr.Textbox(value=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call ChatLLM!\n",
    "\n",
    "Call the model interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build interface\n",
    "with gr.Blocks() as demo:\n",
    "\n",
    "    gr.Markdown(\"## ChatLLM! The right model at the right time\")\n",
    "    chatbot = gr.Chatbot(label=None, show_label=False, placeholder=\"<center><h1><bold>Welcome to ChatLLM</bold></h1>How can I help you?</center>\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(label=None, show_label=False, placeholder=\"Enter your message to ChatLLM\", scale=7)\n",
    "        model = gr.Dropdown(\n",
    "            [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-4\", \"gpt-3.5-turbo\", \"llama3.2\", \"llama3.1\", \"mistral\", \"qwen2.5\", \"qwen2.5-coder\", \"deepseek-r1\"],\n",
    "            label=None,\n",
    "            show_label=False,\n",
    "            value=\"llama3.2\",\n",
    "            scale=2\n",
    "        )\n",
    "\n",
    "    msg.submit(\n",
    "        chat,\n",
    "        inputs=[msg, chatbot, model],\n",
    "        outputs=chatbot\n",
    "    )\n",
    "    \n",
    "    msg.submit(\n",
    "        clear_input,\n",
    "        inputs=[],\n",
    "        outputs=[msg]\n",
    "    )\n",
    "\n",
    "demo.launch(\n",
    "    share=False,\n",
    "    inbrowser=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
